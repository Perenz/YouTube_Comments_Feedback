{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cbIaeP9pX07"
   },
   "source": [
    "# Natural Language Processing - Assignment 2\n",
    "# Sentiment analysis for movie reviews\n",
    "\n",
    "This notebook was created for you to answer question 2, 3 and 4 from assignment 2. Please read the steps and the provided code carefully and make sure you understand them. \n",
    "\n",
    "The (red) comments at the beginning of each function explain what they should do, which parameters you should give as input and which variables should be returned by the function. After the (green) comments \"### student code here###' you should write your own code.\n",
    "\n",
    "**Please modify the next cell specifying your group number**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hIJFbDA1Qbi"
   },
   "source": [
    " *This is the Notebook of* ***Group 35*** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQfxb4pUNs1-"
   },
   "source": [
    "### Prerequisite - Libraries\n",
    "Make sure you have the needed libraries installed on your computer: scikit-learn, Pandas, NLTK..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KiI6RyOpX08"
   },
   "source": [
    "### Prerequisite - Load Data\n",
    "\n",
    "In the first step, we are going to load the data in a Pandas DataFrame. Pandas DataFrames are a useful way of storing data. DataFrames are tables in which data can be accessed as columns, as rows or as individual cells. You can find more info on DataFrames here: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
    "\n",
    "Read the code below and make sure you understand what is happening. Run the code to load your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hX1AE_fJpX09"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/narrietal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eazU-uYcpX1B"
   },
   "outputs": [],
   "source": [
    "def get_path(filename):\n",
    "    \"\"\"\n",
    "    Makes a list of all the paths that fit the search requirement\n",
    "    \n",
    "    :param filename: A regular expression that defines the search requirement for the filenames\n",
    "    :return  Returns a list of all the pathnames\n",
    "    \"\"\"\n",
    "    # place the movies folder in the same directory as this notebook\n",
    "    current_directory = os.getcwd()\n",
    "    # if you are using Google Colab, you will have to change the above line\n",
    "    # to load the dataset from your Google Drive\n",
    "\n",
    "    # glob.glob() is a pattern-matching path finder, it searches for the reviews in the movies folder based on a Regular Expression\n",
    "    paths = glob.glob(current_directory + '/movies/' + filename)\n",
    "    \n",
    "    if len(paths) == 0:\n",
    "        print('Your file list is empty. The code looks for the folder '+current_directory+'/movies, but could not find it.')\n",
    "    else: \n",
    "        print(\"Found \", len(paths), \"files\")\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RrcOjEdSpX1E"
   },
   "outputs": [],
   "source": [
    "def load_data(pathset):\n",
    "    \"\"\"\n",
    "    Loads the data into a dataframe\n",
    "    \n",
    "    :param pathset:  A list of paths\n",
    "    :return  A dataframe with three columns: Path, Review (Text) and Label\n",
    "    \"\"\"\n",
    "    # Files are named by sentiment (P for positive, N for negative)\n",
    "    pattern = re.compile('P-train[0-9]*.txt')\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    df = pd.DataFrame(columns = ['Path', 'Review', 'Label'])\n",
    "    for path in pathset:\n",
    "        if re.search(pattern, path):\n",
    "            text = open(path, \"r\").read()\n",
    "            reviews.append(text)\n",
    "            labels.append('Pos')\n",
    "        else:\n",
    "            text = open(path, \"r\").read()\n",
    "            reviews.append(text)\n",
    "            labels.append('Neg')\n",
    "    df['Path'] = pathset\n",
    "    df['Review'] = reviews\n",
    "    df['Label'] = labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(pathset):\n",
    "    \"\"\"\n",
    "    Loads the data into a dataframe\n",
    "    \n",
    "    :param pathset:  A list of paths\n",
    "    :return  A dataframe with three columns: Path, Review (Text) and Label\n",
    "    \"\"\"\n",
    "    # Files are named by sentiment (P for positive, N for negative)\n",
    "    pattern = re.compile('P-test[0-9]*.txt')\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    df = pd.DataFrame(columns = ['Path', 'Review', 'Label'])\n",
    "    for path in pathset:\n",
    "        if re.search(pattern, path):\n",
    "            text = open(path, \"r\").read()\n",
    "            reviews.append(text)\n",
    "            labels.append('Pos')\n",
    "        else:\n",
    "            text = open(path, \"r\").read()\n",
    "            reviews.append(text)\n",
    "            labels.append('Neg')\n",
    "    df['Path'] = pathset\n",
    "    df['Review'] = reviews\n",
    "    df['Label'] = labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cvGgLWN_pX1G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  600 files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/narrietal/Documents/UT/NLP/HW/HW_2/movie...</td>\n",
       "      <td>with that, carry the same dark weaknesses we a...</td>\n",
       "      <td>Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/narrietal/Documents/UT/NLP/HW/HW_2/movie...</td>\n",
       "      <td>This film, which I rented under the title \"Bla...</td>\n",
       "      <td>Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/narrietal/Documents/UT/NLP/HW/HW_2/movie...</td>\n",
       "      <td>K Murli Mohan Rao made the much better BANDHAN...</td>\n",
       "      <td>Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/narrietal/Documents/UT/NLP/HW/HW_2/movie...</td>\n",
       "      <td>That snarl...\\n\\nThat scowl...\\n\\nThe acts of ...</td>\n",
       "      <td>Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/narrietal/Documents/UT/NLP/HW/HW_2/movie...</td>\n",
       "      <td>This movie was astonishing how good it was! Th...</td>\n",
       "      <td>Pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path  \\\n",
       "0  /home/narrietal/Documents/UT/NLP/HW/HW_2/movie...   \n",
       "1  /home/narrietal/Documents/UT/NLP/HW/HW_2/movie...   \n",
       "2  /home/narrietal/Documents/UT/NLP/HW/HW_2/movie...   \n",
       "3  /home/narrietal/Documents/UT/NLP/HW/HW_2/movie...   \n",
       "4  /home/narrietal/Documents/UT/NLP/HW/HW_2/movie...   \n",
       "\n",
       "                                              Review Label  \n",
       "0  with that, carry the same dark weaknesses we a...   Pos  \n",
       "1  This film, which I rented under the title \"Bla...   Neg  \n",
       "2  K Murli Mohan Rao made the much better BANDHAN...   Neg  \n",
       "3  That snarl...\\n\\nThat scowl...\\n\\nThe acts of ...   Neg  \n",
       "4  This movie was astonishing how good it was! Th...   Pos  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the files in the Dataframe. This will take a while...\n",
    "paths = get_path('train/[NP]-train[0-9]*.txt')\n",
    "data = load_data(paths)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRRamA_8pX1K"
   },
   "source": [
    "### Part 2 - Tokenization\n",
    "\n",
    "In this step, you should write a tokenizer and compare it with an off-the-shelf one.\n",
    "\n",
    "#### Question 2.1 Making your own tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "UkZwy1ATNs2F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', ' ', 'you', ' ', 'have', ' ', 'the', ' ', 'chance', ',', ' ', 'watch', ' ', 'it', '.', ' ', 'Although', ',', ' ', 'a', ' ', 'warning', ',', ' ', 'you', \"'\", 'll', ' ', 'cry', ' ', 'your', ' ', 'eyes', ' ', 'out', '.']\n",
      "['I', ' ', 'hope', ' ', 'this', ' ', 'email', ' ', 'finds', ' ', 'you', ' ', 'well', ' ', 'Anna', '.', ' ', 'I', ' ', 'know', ' ', 'it', \"'\", 's', ' ', 'been', ' ', 'tough', ' ', 'times', '.']\n",
      "['Hey', ' ', 'Thomas', '!', ' ', 'How', ' ', 'is', ' ', 'it', ' ', 'going', '?', ' ', 'I', ' ', 'thought', ' ', 'you', ' ', 'were', ' ', 'studying', ' ', 'abroad', '.']\n",
      "['Please', ',', ' ', 'mind', ' ', 'the', ' ', 'gap', ' ', 'between', ' ', 'the', ' ', 'train', ' ', 'and', ' ', 'the', ' ', 'platform', '.']\n"
     ]
    }
   ],
   "source": [
    "def my_tokenizer(text):\n",
    "    \"\"\"\n",
    "    The implementation of your own tokenizer\n",
    "    \n",
    "    :param text:  A string with a sentence (or paragraph, or document...)\n",
    "    :return  A list of tokens\n",
    "    \"\"\"    \n",
    "    \n",
    "    tokenized_text = re.findall(r\"[\\w]+|[^\\s\\w]\", text)\n",
    "    \n",
    "    return tokenized_text\n",
    "\n",
    "sample_string0 = \"If you have the chance, watch it. Although, a warning, you'll cry your eyes out.\"\n",
    "sample_string1 = \"I hope this email finds you well Anna. I know it's been tough times.\"\n",
    "sample_string2 = \"Hey Thomas! How is it going? I thought you were studying abroad.\" \n",
    "sample_string3 = \"Please, mind the gap between the train and the platform.\"\n",
    "print(my_tokenizer(sample_string0))\n",
    "print(my_tokenizer(sample_string1))\n",
    "print(my_tokenizer(sample_string2))\n",
    "print(my_tokenizer(sample_string3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pxI0gdoNs2G"
   },
   "source": [
    "#### Question 2.2 Using an off-the-shelf tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TXUTKVyqNs2H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'this', 'assignment', 'because', ':', '-', 'it', 'is', 'fun', ';', '-', 'it', 'helps', 'me', 'practice', 'my', 'Python', 'skills', '.']\n",
      "['I', 'like', 'this', 'assignment', 'because', ':', '-', 'it', 'is', 'fun', ';', '-', 'it', 'helps', 'me', 'practice', 'my', 'Python', 'skills', '.']\n",
      "\n",
      "\n",
      "['I', 'won', 'a', 'prize', ',', 'but', 'I', 'won', \"'\", 't', 'be', 'able', 'to', 'attend', 'the', 'ceremony', '.']\n",
      "['I', 'won', 'a', 'prize', ',', 'but', 'I', 'wo', \"n't\", 'be', 'able', 'to', 'attend', 'the', 'ceremony', '.']\n",
      "\n",
      "\n",
      "['“', 'The', 'strange', 'case', 'of', 'Dr', '.', 'Jekyll', 'and', 'Mr', '.', 'Hyde', '”', 'is', 'a', 'famous', 'book', '.', '.', '.', 'but', 'I', 'haven', \"'\", 't', 'read', 'it', '.']\n",
      "['“', 'The', 'strange', 'case', 'of', 'Dr.', 'Jekyll', 'and', 'Mr.', 'Hyde', '”', 'is', 'a', 'famous', 'book', '...', 'but', 'I', 'have', \"n't\", 'read', 'it', '.']\n",
      "\n",
      "\n",
      "['I', 'work', 'for', 'the', 'C', '.', 'I', '.', 'A', '.', '.', 'And', 'you', '?']\n",
      "['I', 'work', 'for', 'the', 'C.I.A', '..', 'And', 'you', '?']\n",
      "\n",
      "\n",
      "['OMG', '#', 'Twitter', 'is', 'sooooo', 'coooool', '<', '3', ':', '-', ')', '<', '-', '-', 'lol', '.', '.', '.', 'why', 'do', 'i', 'write', 'like', 'this', 'idk', 'right', '?', ':', ')', '🤷', '😂', '🤖']\n",
      "['OMG', '#', 'Twitter', 'is', 'sooooo', 'coooool', '<', '3', ':', '-', ')', '<', '--', 'lol', '...', 'why', 'do', 'i', 'write', 'like', 'this', 'idk', 'right', '?', ':', ')', '🤷😂', '🤖']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now we are gonna compare the tokenizer you just wrote with the one from NLTK\n",
    "#if you installed NLTK but never downloaded the 'punkt' tokenizer, uncomment the following lines:\n",
    "def nltk_tokenizer(text):\n",
    "    \"\"\"\n",
    "    This function should apply the word_tokenize (punkt) tokenizer of nltk to the input text\n",
    "    \n",
    "    :param text:  A string with a sentence (or paragraph, or document...)\n",
    "    :return  A list of tokens\n",
    "    \"\"\"     \n",
    "      \n",
    "    tokenized_text = word_tokenize(text)\n",
    "    \n",
    "    \n",
    "    return tokenized_text\n",
    "\n",
    "test_sentences = [\"I like this assignment because:\\n-\\tit is fun;\\n-\\tit helps me practice my Python skills.\",\n",
    "        \"I won a prize, but I won't be able to attend the ceremony.\",\n",
    "        \"“The strange case of Dr. Jekyll and Mr. Hyde” is a famous book... but I haven't read it.\",\n",
    "        \"I work for the C.I.A.. And you?\",\n",
    "        \"OMG #Twitter is sooooo coooool <3 :-) <-- lol...why do i write like this idk right? :) 🤷😂 🤖\"]\n",
    "\n",
    "for test_string in test_sentences:\n",
    "    print(my_tokenizer(test_string))\n",
    "    print(nltk_tokenizer(test_string))\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUrQ_8EbNs2N"
   },
   "source": [
    "### Part 3 - Text classification with a unigram language model\n",
    "\n",
    "#### Training phase\n",
    "You now need to create the model and train it on the documents in the dataframe. Look at the scikit learn documentation to learn how to use the CountVectorizer and MultimodalNaiveBayes modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tQMy8K-MNs2N"
   },
   "outputs": [],
   "source": [
    "count_vector_standard = CountVectorizer()\n",
    "count_vector_stop_words = CountVectorizer(stop_words=\"english\")\n",
    "count_vector_bigram = CountVectorizer(ngram_range=(2,2))\n",
    "count_vector_trigram = CountVectorizer(ngram_range=(3,3))\n",
    "\n",
    "\n",
    "def vectorize(reviews, stop_words=False, test=False, ngram_type='unigram'):\n",
    "    reviews_array=reviews.to_numpy().flatten()\n",
    "    \n",
    "    if stop_words:\n",
    "        count_vector = count_vector_stop_words\n",
    "        if test:\n",
    "            count_matrix = count_vector.transform(reviews_array)\n",
    "        else:\n",
    "            count_matrix = count_vector.fit_transform(reviews_array)\n",
    "\n",
    "    else:\n",
    "        if ngram_type == 'bigram':\n",
    "            count_vector = count_vector_bigram\n",
    "        elif ngram_type == 'trigram':\n",
    "            count_vector = count_vector_trigram\n",
    "        else:\n",
    "            count_vector = count_vector_standard\n",
    "            \n",
    "        if test:\n",
    "            count_matrix = count_vector.transform(reviews_array)\n",
    "        else:\n",
    "            count_matrix = count_vector.fit_transform(reviews_array)\n",
    "\n",
    "    count_array = count_matrix.toarray()\n",
    "    df = pd.DataFrame(data=count_array,columns = count_vector.get_feature_names())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard model\n",
    "train_data_vectorized = vectorize(data['Review'])\n",
    "NB_model = MultinomialNB().fit(train_data_vectorized, data['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrmIrKsbNs2O"
   },
   "source": [
    "#### Testing phase\n",
    "Now that you have a trained model, you need to test its performance.\n",
    "\n",
    "1. Load your test data.\n",
    "2. Classify your test data using the classifier you trained before.\n",
    "3. Compute the accuracy of your classifier on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NNZtd9aqNs2O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  50 files\n"
     ]
    }
   ],
   "source": [
    "paths = get_path('test/[NP]-test[0-9]*.txt')\n",
    "test_data = load_test_data(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_vectorized = vectorize(test_data['Review'],test=True)\n",
    "y_predicted = NB_model.predict(test_data_vectorized)\n",
    "y_true = test_data['Label']\n",
    "accuracy_score(y_true, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2XPTWi1ytLW"
   },
   "source": [
    "Now train two more models: one without Laplace smoothing, and one where stopwords are removed. Then test them on the same test data, and compare the performance with the results you previously obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "H6antoDczL0U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model with out smoothing 0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/narrietal/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    }
   ],
   "source": [
    "#Model without smoothing:\n",
    "NB_model_no_smoothing = MultinomialNB(alpha=0).fit(train_data_vectorized, data['Label'])\n",
    "y_predicted_no_smoothing = NB_model_no_smoothing.predict(test_data_vectorized)\n",
    "acc_no_smooth = accuracy_score(y_true, y_predicted_no_smoothing)\n",
    "print(\"Accuracy of model with out smoothing\", acc_no_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model with stop words 0.74\n"
     ]
    }
   ],
   "source": [
    "#Model with stop words removed:\n",
    "train_data_vectorized_stop_words = vectorize(data['Review'], stop_words=True)\n",
    "test_data_vectorized_stop_words = vectorize(test_data['Review'],stop_words=True, test=True)\n",
    "\n",
    "NB_model_stop_words = MultinomialNB().fit(train_data_vectorized_stop_words, data['Label'])\n",
    "y_predicted_stop_words = NB_model_stop_words.predict(test_data_vectorized_stop_words)\n",
    "acc_stop_words = accuracy_score(y_true, y_predicted_stop_words)\n",
    "print(\"Accuracy of model with stop words\", acc_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDCgEfYgNs2Q"
   },
   "source": [
    "### Part 4 - Text classification with a bigram language model\n",
    "\n",
    "Now we will classify the same dataset again, but this time with a bigram language model. \n",
    "\n",
    "#### Training phase\n",
    "Build a Naïve Bayes classifier that uses bigrams instead of single words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ZSIam3ObNs2Q"
   },
   "outputs": [],
   "source": [
    "#Model with bigrams:\n",
    "train_data_vectorized_bigrams = vectorize(data['Review'], ngram_type='bigram')\n",
    "test_data_vectorized_bigrams = vectorize(test_data['Review'],test=True, ngram_type='bigram')\n",
    "\n",
    "NB_model_bigram = MultinomialNB().fit(train_data_vectorized_bigrams, data['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReyUDT1dNs2R"
   },
   "source": [
    "#### Testing phase\n",
    "As before, calculate the performance on your test data, and notice the difference with the previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "z6rkqDJENs2R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model with bigrams 0.74\n"
     ]
    }
   ],
   "source": [
    "y_predicted_bigram = NB_model_bigram.predict(test_data_vectorized_bigrams)\n",
    "acc_bigrams = accuracy_score(y_true, y_predicted_bigram)\n",
    "print(\"Accuracy of model with bigrams\", acc_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gY3K9vaB0Vzk"
   },
   "source": [
    "### Trigrams\n",
    "When I asked students how to improve the classification performance on this dataset, the first question was always \"use trigrams\" (or even higher-order n-grams). Let's try how much of an improvement that would be, by training a trigram model and testing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "U7htbTfeNs2S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model with trigrams 0.6\n"
     ]
    }
   ],
   "source": [
    "#Model with trigrams:\n",
    "train_data_vectorized_trigrams = vectorize(data['Review'], ngram_type='trigram')\n",
    "test_data_vectorized_trigrams = vectorize(test_data['Review'],test=True, ngram_type='trigram')\n",
    "\n",
    "NB_model_trigram = MultinomialNB().fit(train_data_vectorized_trigrams, data['Label'])\n",
    "y_predicted_trigram = NB_model_trigram.predict(test_data_vectorized_trigrams)\n",
    "acc_trigrams = accuracy_score(y_true, y_predicted_trigram)\n",
    "print(\"Accuracy of model with trigrams\", acc_trigrams)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_2021_Homework2_FINAL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
