{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reflected-confirmation",
   "metadata": {},
   "source": [
    "# Youtube comment's sentiment analysis\n",
    "\n",
    "Authors: Stefano Perenzoni & Nicolás Arrieta Larraza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-literature",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is part of the project named \"Topic modelling and sentiment analysis of Youtube comments\" and it contains our sentiment analysis approach on a british Youtube music videos comment's dataset. \n",
    "\n",
    "We mention more of the dataset in previous notebooks, thus, what it matters for this notebook is that the already-cleaned and filtered datase contains 51790 unlabebeled comments in english.\n",
    "\n",
    "For such an unsupervised task, in the following sections we present:\n",
    "* An implementation of our own sentiment analysis classifier that includes:\n",
    "    * Data preprocessing\n",
    "    \n",
    "    * Word embedding (Word2Vec)\n",
    "    \n",
    "    * Clustering of words (K-Means)\n",
    "    \n",
    "    * Sentiment prediction based on TF-iDF and closeness scores of the words\n",
    "    \n",
    "    \n",
    "* An analysis and comparison of more suitable models for this scenerario, which include: \n",
    "\n",
    "    * Sentiment analysis with Vader: A rule-based sentiment analysis tool tuned for social media content\n",
    "\n",
    "    * Sentiment analysis with TextBlob: A rule-based sentiment analysis tool with Subjectivity analysis\n",
    "\n",
    "    * Sentiment analysis with Flair: An embedding-based NLP framework with pre-trained sentiment analysis models\n",
    "    \n",
    "    \n",
    "* A final approach with Ensemble Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-tuesday",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pleasant-arctic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/narrietal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from time import time\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import flair\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import spacy \n",
    "from scipy.spatial import distance\n",
    "import multiprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "data_path = '../../data/'\n",
    "models_path = '../../data/models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-princeton",
   "metadata": {},
   "source": [
    "## Importing already filtered comments\n",
    "We already filtered the coments from the initial raw dataset as follows:\n",
    "* Keep only british comments\n",
    "* Removing missing cells\n",
    "* Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "proper-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing filtered comments\n",
    "comments_df = pd.read_csv(data_path+'GB_comments_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "meaning-absence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I didn't really like the song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alright... I'll take the stairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a fable on a realistic ground  is a cry of pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hope you will exist for many years..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Would this song exist if he lived on the first...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text\n",
       "0                      I didn't really like the song\n",
       "1                    alright... I'll take the stairs\n",
       "2  a fable on a realistic ground  is a cry of pro...\n",
       "3               hope you will exist for many years..\n",
       "4  Would this song exist if he lived on the first..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting only the comments \n",
    "comments_text_df = comments_df['comment_text'].to_frame()\n",
    "comments_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "external-program",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I didn't really like the song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alright... I'll take the stairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a fable on a realistic ground  is a cry of pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hope you will exist for many years..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Would this song exist if he lived on the first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51785</th>\n",
       "      <td>Whatever is related to him is = PURE PERFECTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51786</th>\n",
       "      <td>Zhu is awesome! Unfortunately, Adidas must of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51787</th>\n",
       "      <td>Once again the Asians dominate!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51788</th>\n",
       "      <td>ZHU is a fucking magical aesthetic unicorn god</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51789</th>\n",
       "      <td>sold out to a global corp of scumbags</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51790 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comment_text\n",
       "0                          I didn't really like the song\n",
       "1                        alright... I'll take the stairs\n",
       "2      a fable on a realistic ground  is a cry of pro...\n",
       "3                   hope you will exist for many years..\n",
       "4      Would this song exist if he lived on the first...\n",
       "...                                                  ...\n",
       "51785    Whatever is related to him is = PURE PERFECTION\n",
       "51786  Zhu is awesome! Unfortunately, Adidas must of ...\n",
       "51787                    Once again the Asians dominate!\n",
       "51788     ZHU is a fucking magical aesthetic unicorn god\n",
       "51789              sold out to a global corp of scumbags\n",
       "\n",
       "[51790 rows x 1 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-contrast",
   "metadata": {},
   "source": [
    "## Implementing our own sentiment analysis classifier\n",
    "\n",
    "First, we attempt to implement our own classifier. We can foresee that it is a complex challenge as it is an unsupervised task on a dataset with a wide variety of comments.\n",
    "\n",
    "Anyway, we believe it is interesting to develop this classic approach in order to compare it with the performance of more suitable models for this task.\n",
    "\n",
    "In the following sections we proceed to do the tasks listed below:\n",
    "* Preprocess the data: Clean, tokenize and lemmatize the comments\n",
    "* Detect common phrases\n",
    "* Create Word2Vec model\n",
    "* Apply K-Means algorithm\n",
    "* Estimate comment's sentiment analysis by computing the dot product of the tf-idf scores and sentiment analaysis scores of every word in the comment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-making",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-definition",
   "metadata": {},
   "source": [
    "Keeping only alphanumerical words, tokenizing, lemmatizing and erasing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "regulated-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def comment_to_word_list(comment):\n",
    "    # Keeps only alpha numerical tokens\n",
    "    comment = re.sub(r\"[^A-Za-z']+\", ' ', str(comment).lower())\n",
    "    # Divides comment in tokens\n",
    "    comment = word_tokenize(comment)\n",
    "    # Lemmatizes and removes stopwords\n",
    "    comment = [lemmatizer.lemmatize(token) for token in comment if not token in stop_words]\n",
    "    \n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "periodic-favorite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning time: 0.31 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "clean_comments_df = comments_text_df.copy()\n",
    "\n",
    "clean_comments_df.comment_text = clean_comments_df.comment_text.apply(lambda x: comment_to_word_list(x))\n",
    "\n",
    "print('Cleaning time: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-longitude",
   "metadata": {},
   "source": [
    "### Bigram \n",
    "\n",
    "We use the Gensim Phrases to automatically detect common phrases from a given list of sentences. (E.g. \"New York\" would be returned as \"New_York\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "grave-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [word for word in clean_comments_df.comment_text]\n",
    "phrases = Phrases(sent, min_count=30, progress_per=10000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-spencer",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "We are going to embed the commets in 300 dimensional vectors with Word2Vec. We use the following parameters for the vectorization:\n",
    "\n",
    "* min count = 3 - remove most unusual words from training embeddings (words that appear less than 3 times)\n",
    "* window = 4 - Word2Vec model will learn to predict given word from up to 4 words to the left, and up to 4 words to the right\n",
    "* vector_size = 300 - size of hidden layer used to predict surroundings of embedded word, which also stands for dimensions of trained embeddings\n",
    "* sample = 1e-5 - probability baseline for subsampling most frequent words from surrounding of embedded word\n",
    "* alpha = learning rate\n",
    "* min_alpha = learning rate will decay to this value\n",
    "* negative = 20 - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "exceptional-anchor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.02 mins\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=3,\n",
    "                     window=4,\n",
    "                     vector_size=300,\n",
    "                     sample=1e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)\n",
    "\n",
    "start = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=50000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - start) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "infrared-database",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.68 mins\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - start) / 60, 2)))\n",
    "\n",
    "w2v_model.save(models_path+\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-lyric",
   "metadata": {},
   "source": [
    "### K-Means\n",
    "\n",
    "K-Means it is an iterative clustering algorithm, in which in first step K random data points are chosen as coordinates of clusters centroids (where n is the number of seeked clusters). In every step all points are assigned to their closest centroid, based on euclidean distance. Then, new coordinates of every centroid are calculated, as mean of coordinates of all data points assigned to each centroid, and iterations are repeated till reaching minimal value of squared sum of distances between points assigned to centroids, and their centroid coordinates (which just simply means that coordinates of clusters stop to change), or number of iterations reach given limit.\n",
    "\n",
    "For our task, we choose K=50 clusters and a stoping criteria of 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "charming-speed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compute K-Means algorithm: 0.12 mins\n"
     ]
    }
   ],
   "source": [
    "word_vectors = w2v_model.wv\n",
    "start = time()\n",
    "kmeans_model = KMeans(n_clusters=2, max_iter=1000, n_init=50)\n",
    "kmeans_model.fit(X=word_vectors.vectors.astype('double'))\n",
    "\n",
    "print('Time to compute K-Means algorithm: {} mins'.format(round((time() - start) / 60, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-pierre",
   "metadata": {},
   "source": [
    "#### Discussing results\n",
    "\n",
    "In order to understand which cluster belongs to positive or negative, we take a look at the words that are closest, in terms of cosine similarity, to the coordinates of the centroids of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tracked-edward",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('juss', 0.9997978806495667),\n",
       " ('freshly', 0.9997945427894592),\n",
       " ('chipmunk', 0.9997785687446594),\n",
       " ('racial', 0.9997736215591431),\n",
       " ('lotta', 0.9997729659080505),\n",
       " ('guccimane', 0.999771237373352),\n",
       " ('heared', 0.9997678399085999),\n",
       " ('wholesome', 0.9997676014900208),\n",
       " ('earthquake', 0.9997669458389282),\n",
       " ('tm', 0.9997663497924805)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_vector(kmeans_model.cluster_centers_[0], topn=10, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "little-logging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ntake', 0.9858958721160889),\n",
       " ('nhow', 0.9841887950897217),\n",
       " ('nwithout', 0.9833437204360962),\n",
       " ('nwill', 0.981657087802887),\n",
       " ('nare', 0.9787806868553162),\n",
       " ('nam', 0.9782816767692566),\n",
       " ('say', 0.976513683795929),\n",
       " ('mind', 0.9759274125099182),\n",
       " ('unsure', 0.9748870730400085),\n",
       " ('try', 0.9748343825340271)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_vector(kmeans_model.cluster_centers_[1], topn=10, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-thirty",
   "metadata": {},
   "source": [
    "We can observe that the 2 closest words to the centroid of the first cluster are \"goooood\" and \"wholesome\". This could suggest that such a cluster could be the positive one.\n",
    "\n",
    "However, based on the closest words, it is not clear if the second cluster belongs to negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-trouble",
   "metadata": {},
   "source": [
    "We can also check for certain words to see to which cluster they have been assigned to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "collect-preservation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word good was assigned to cluster: 0 at a distance from its centroid of: 0.026116818818596843 \n",
      "\n",
      "The word bad was assigned to cluster: 0 at a distance from its centroid of: 0.08549509233707464 \n",
      "\n",
      "The word wrong was assigned to cluster: 1 at a distance from its centroid of: 0.09559552489929268 \n",
      "\n",
      "The word great was assigned to cluster: 0 at a distance from its centroid of: 0.05570642855848751 \n",
      "\n",
      "The word terrible was assigned to cluster: 0 at a distance from its centroid of: 0.012777189218619145 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_words = ['good', 'bad', 'wrong', \"great\", \"terrible\"]\n",
    "for w in test_words:\n",
    "    cluster_index = kmeans_model.labels_[word_vectors.key_to_index[w]]\n",
    "    print('The word', w, 'was assigned to cluster:', cluster_index, 'at a distance from its centroid of:',distance.cosine(word_vectors[w], kmeans_model.cluster_centers_[cluster_index]),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-survivor",
   "metadata": {},
   "source": [
    "As we can observe, the results seem to show that the clusters are not dividing correctly the words in terms of positive/negative sentiment. The words \"good\",\"bad\",\"great\" and \"terrible\" are assigned to the same, while \"wrong\" it is assigned differently.\n",
    "\n",
    "We can guess that the final result will not be accurate, but we will continue with the task assuming that the first cluster belongs to positive words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-spending",
   "metadata": {},
   "source": [
    "#### Assigning clusters\n",
    "\n",
    "We will assign to each word a sentiment score, -1 or 1, depending on the cluster they belong to, negative or positive respectively. To get this score we multiply it by the distance to the centroid of their cluster. As the score that K-means algorithm outputs is distance from both clusters, to properly weigh them we multiplied them by the inverse of closeness score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ultimate-partner",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(word_vectors.index_to_key)\n",
    "words.columns = ['words']\n",
    "words['vectors'] = words.words.apply(lambda x: word_vectors[f'{x}'])\n",
    "words['cluster'] = words.vectors.apply(lambda x: kmeans_model.predict([np.array(x)]))\n",
    "words.cluster = words.cluster.apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "governmental-homeless",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_value</th>\n",
       "      <th>closeness_score</th>\n",
       "      <th>sentiment_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>song</td>\n",
       "      <td>[0.0847291, -0.046834808, -0.031612296, 0.0978...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.950590</td>\n",
       "      <td>0.950590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>[0.18501046, 0.11311181, 0.0032570814, -0.0501...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.556316</td>\n",
       "      <td>-0.556316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'s</td>\n",
       "      <td>[0.04604786, -0.050572217, -0.061958257, 0.062...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.656895</td>\n",
       "      <td>-0.656895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n</td>\n",
       "      <td>[0.014859654, -0.17155193, -0.14537424, 0.1939...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.389754</td>\n",
       "      <td>-0.389754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>like</td>\n",
       "      <td>[0.09511839, 0.0051237526, -0.031544328, 0.093...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.863353</td>\n",
       "      <td>0.863353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  words                                            vectors  cluster  \\\n",
       "0  song  [0.0847291, -0.046834808, -0.031612296, 0.0978...        0   \n",
       "1  love  [0.18501046, 0.11311181, 0.0032570814, -0.0501...        1   \n",
       "2    's  [0.04604786, -0.050572217, -0.061958257, 0.062...        1   \n",
       "3     n  [0.014859654, -0.17155193, -0.14537424, 0.1939...        1   \n",
       "4  like  [0.09511839, 0.0051237526, -0.031544328, 0.093...        0   \n",
       "\n",
       "   cluster_value  closeness_score  sentiment_coeff  \n",
       "0              1         0.950590         0.950590  \n",
       "1             -1         0.556316        -0.556316  \n",
       "2             -1         0.656895        -0.656895  \n",
       "3             -1         0.389754        -0.389754  \n",
       "4              1         0.863353         0.863353  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words['cluster_value'] = [1 if i==0 else -1 for i in words.cluster]\n",
    "words['closeness_score'] = words.apply(lambda x: 1/(kmeans_model.transform([x.vectors]).min()), axis=1)\n",
    "words['sentiment_coeff'] = words.closeness_score * words.cluster_value\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "alone-cutting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiment_dict_df = words[['words', 'sentiment_coeff']]\n",
    "sentiment_dict = {row['words']:row['sentiment_coeff'] for index,row in sentiment_dict_df.iterrows()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-benjamin",
   "metadata": {},
   "source": [
    "### Sentiment prediction\n",
    "\n",
    "The last step is to predict the sentiment of every comment.\n",
    "\n",
    "For that we first produce a vector with the tf-idf score of every word in a sentence. This gives more weight to the words that are more important in a sentence.\n",
    "\n",
    "Then, we produce a second vector with the sentiment scores that we computed in the previous section for the same words in a sentence.\n",
    "\n",
    "In order to get the final sentiment prediction score for that sentence, we compute the dot product of the vectors. If the dot product is positive, the sentiment is positive, otherwise is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "irish-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None)\n",
    "\n",
    "tfidf.fit(clean_comments_df.comment_text)\n",
    "features = pd.Series(tfidf.get_feature_names())\n",
    "transformed = tfidf.transform(clean_comments_df.comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "artistic-pocket",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to create tf-idf and closeness vectors: 0.37 mins\n"
     ]
    }
   ],
   "source": [
    "def create_tfidf_dictionary(x, transformed_file, features):\n",
    "    vector_coo = transformed_file[x.name].tocoo()\n",
    "    vector_coo.col = features.iloc[vector_coo.col].values\n",
    "    dict_from_coo = dict(zip(vector_coo.col, vector_coo.data))\n",
    "    return dict_from_coo\n",
    "\n",
    "def replace_tfidf_words(x, transformed_file, features):\n",
    "    dictionary = create_tfidf_dictionary(x, transformed_file, features)   \n",
    "    return list(map(lambda y:dictionary[f'{y}'], x.comment_text))\n",
    "\n",
    "\n",
    "def replace_sentiment_words(word, sentiment_dict):\n",
    "    try:\n",
    "        out = sentiment_dict[word]\n",
    "    except KeyError:\n",
    "        out = 0\n",
    "    return out\n",
    "\n",
    "start = time()\n",
    "replaced_tfidf_scores = clean_comments_df.apply(lambda x: replace_tfidf_words(x, transformed, features),axis=1)\n",
    "replaced_closeness_scores = clean_comments_df.comment_text.apply(lambda x: list(map(lambda y: replace_sentiment_words(y, sentiment_dict), x)))\n",
    "print('Time to create tf-idf and closeness vectors: {} mins'.format(round((time() - start) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "modified-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = pd.DataFrame(data=[comments_text_df.comment_text, replaced_closeness_scores, replaced_tfidf_scores]).T\n",
    "sentiment_df.columns = ['comment_text','sentiment_coeff', 'tfidf_scores']\n",
    "sentiment_df['sentiment_rate'] = sentiment_df.apply(lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']), axis=1)\n",
    "sentiment_df['prediction'] = (sentiment_df.sentiment_rate>0).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "broad-marina",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>sentiment_coeff</th>\n",
       "      <th>tfidf_scores</th>\n",
       "      <th>sentiment_rate</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I didn't really like the song</td>\n",
       "      <td>[-0.8887895303099768, 0.5568657165960161, 0.86...</td>\n",
       "      <td>[0.49077125350687817, 0.6206134296985032, 0.46...</td>\n",
       "      <td>0.687134</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alright... I'll take the stairs</td>\n",
       "      <td>[1.621816551909419, -0.37983587693121174, 0.53...</td>\n",
       "      <td>[0.5612342998154032, 0.39085294001535686, 0.38...</td>\n",
       "      <td>8.743038</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a fable on a realistic ground  is a cry of pro...</td>\n",
       "      <td>[0, 9.191734999382218, -0.5380640659852599, -0...</td>\n",
       "      <td>[0.3803346994119293, 0.35037504803931013, 0.28...</td>\n",
       "      <td>11.256144</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hope you will exist for many years..</td>\n",
       "      <td>[0.9417209272109989, 5.508054667285054, 1.3563...</td>\n",
       "      <td>[0.43309359691929417, 0.6754506092727891, 0.43...</td>\n",
       "      <td>5.378246</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Would this song exist if he lived on the first...</td>\n",
       "      <td>[1.00602934646922, 0.950590108676847, 5.508054...</td>\n",
       "      <td>[0.30266359901576395, 0.176829989718542, 0.540...</td>\n",
       "      <td>5.489792</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0                      I didn't really like the song   \n",
       "1                    alright... I'll take the stairs   \n",
       "2  a fable on a realistic ground  is a cry of pro...   \n",
       "3               hope you will exist for many years..   \n",
       "4  Would this song exist if he lived on the first...   \n",
       "\n",
       "                                     sentiment_coeff  \\\n",
       "0  [-0.8887895303099768, 0.5568657165960161, 0.86...   \n",
       "1  [1.621816551909419, -0.37983587693121174, 0.53...   \n",
       "2  [0, 9.191734999382218, -0.5380640659852599, -0...   \n",
       "3  [0.9417209272109989, 5.508054667285054, 1.3563...   \n",
       "4  [1.00602934646922, 0.950590108676847, 5.508054...   \n",
       "\n",
       "                                        tfidf_scores  sentiment_rate  \\\n",
       "0  [0.49077125350687817, 0.6206134296985032, 0.46...        0.687134   \n",
       "1  [0.5612342998154032, 0.39085294001535686, 0.38...        8.743038   \n",
       "2  [0.3803346994119293, 0.35037504803931013, 0.28...       11.256144   \n",
       "3  [0.43309359691929417, 0.6754506092727891, 0.43...        5.378246   \n",
       "4  [0.30266359901576395, 0.176829989718542, 0.540...        5.489792   \n",
       "\n",
       "   prediction  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "martial-limitation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of prediction')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sentiment_df.prediction)\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "vulnerable-xerox",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    50344\n",
       "0     1446\n",
       "Name: prediction, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df['prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-ranch",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "We can observe that, as it was expected from the little promising results of the K-Means algorithm, the model does not classify correctly the comments.\n",
    "\n",
    "The last histogram clearly shows that 97% of the instances belong to the \"positive\" cluster. This is undoubtly a wrong result since we know from the clustering section that negative words like \"bad\" or \"terrible\" were assigned a positive score.\n",
    "\n",
    "Further development could be done to attempt achieving better results such as: Including 3rd \"Neutral\" cluster, computing K-means with cosine similarity, changing word2vector parameters, etc.\n",
    "\n",
    "Given the results, seems like performing such a task through this approach might not be the best solution. Therefore, in the following sections, we will attempt the sentiment analysis tasks with three different approaches:\n",
    "\n",
    "* Sentiment analysis with Vader: A rule-based sentiment analysis tool tuned for social media content\n",
    "\n",
    "* Sentiment analysis with TextBlob: A rule-based sentiment analysis tool with Subjectivity analysis\n",
    "\n",
    "* Sentiment analysis with Flair: An embedding-based NLP framework with pre-trained sentiment analysis models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-houston",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Vader\n",
    "\n",
    "Vader (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\n",
    "\n",
    "The scores provided by Vader are the following:\n",
    "\n",
    "* Neg, Neu and Pos are a proportional score oh how many negative, neutral or positive words are found in a sentence. They should sum up to 1.\n",
    "\n",
    "* Compound is the score computed by summing the scores of each word in the lexicon and normalized. Thus, the value is bounded between -1 (most negative) and 1 (most positive).\n",
    "    * If compound score <=  -0.05 NEGATIVE\n",
    "    * If compound score > -0.05 and <0.05 NEUTRAL\n",
    "    * If compound score >=  0.05 POSITIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "sharp-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "approximate-family",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader_analyser.polarity_scores(\"aaaaa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "unsigned-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_scores = []\n",
    "for row in comments_text_df.comment_text:\n",
    "    score = vader_analyser.polarity_scores(row)\n",
    "    vader_scores.append(score)\n",
    "\n",
    "#Converting List of Dictionaries into Dataframe\n",
    "vader_scores_df= pd.DataFrame(vader_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "secret-program",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I didn't really like the song</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.3241</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alright... I'll take the stairs</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a fable on a realistic ground  is a cry of pro...</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.5106</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hope you will exist for many years..</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Would this song exist if he lived on the first...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text    neg    neu    pos  \\\n",
       "0                      I didn't really like the song  0.318  0.682  0.000   \n",
       "1                    alright... I'll take the stairs  0.000  0.667  0.333   \n",
       "2  a fable on a realistic ground  is a cry of pro...  0.223  0.699  0.079   \n",
       "3               hope you will exist for many years..  0.000  0.674  0.326   \n",
       "4  Would this song exist if he lived on the first...  0.000  1.000  0.000   \n",
       "\n",
       "   compound  prediction  \n",
       "0   -0.3241          -1  \n",
       "1    0.2500           1  \n",
       "2   -0.5106          -1  \n",
       "3    0.4404           1  \n",
       "4    0.0000           0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader_rated_comments_df = pd.concat([comments_text_df, vader_scores_df], axis=1)\n",
    "vader_rated_comments_df['Prediction'] = [1 if i>0.05 else -1 if i<0.05 else 0 for i in vader_rated_comments_df.compound]\n",
    "vader_rated_comments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-management",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with TextBlob\n",
    "\n",
    "TextBlov is another rule-based sentiment analysis tool in which sentiments are defined based on semantic relations and the frequency of each word in an input sentence.\n",
    "\n",
    "The scores provided by TextBob are the following:\n",
    "\n",
    "* Polarity score: float within the range [-1.0, 1.0] where -1.0 is a negative polarity and 1.0 is positive. This score can also be equal to 0, which stands for a neutral evaluation\n",
    "\n",
    "* Subjectivity score: loat within the range [0.0, 1.0] where 0.0 is a very objective sentence and 1.0 is very subjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "lesser-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "textblob_scores = []\n",
    "for row in comments_text_df.comment_text:\n",
    "    score = {}\n",
    "    score['Polarity'] = TextBlob(row).sentiment.polarity\n",
    "    score['Subjectivity'] = TextBlob(row).sentiment.subjectivity\n",
    "    textblob_scores.append(score)\n",
    "\n",
    "#Converting List of Dictionaries into Dataframe\n",
    "text_blob_scores_df= pd.DataFrame(textblob_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dominican-relief",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I didn't really like the song</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alright... I'll take the stairs</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a fable on a realistic ground  is a cry of pro...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hope you will exist for many years..</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Would this song exist if he lived on the first...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  Polarity  Subjectivity  \\\n",
       "0                      I didn't really like the song  0.200000      0.200000   \n",
       "1                    alright... I'll take the stairs  0.000000      0.000000   \n",
       "2  a fable on a realistic ground  is a cry of pro...  0.166667      0.333333   \n",
       "3               hope you will exist for many years..  0.500000      0.500000   \n",
       "4  Would this song exist if he lived on the first...  0.250000      0.333333   \n",
       "\n",
       "   prediction  \n",
       "0           1  \n",
       "1           0  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textblob_rated_comments_df = pd.concat([comments_text_df, text_blob_scores_df], axis=1)\n",
    "textblob_rated_comments_df['Prediction'] = [1 if i>0 else -1 if i<0 else 0 for i in textblob_rated_comments_df.Polarity]\n",
    "textblob_rated_comments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-marker",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Flair\n",
    "\n",
    "Flair is an state-of-the-art NLP framework that provides, among other things, a sentiment analysis tool. Flair’s sentiment classifier is based on a character-level LSTM neural network which takes sequences of letters and words into account when predicting.\n",
    "\n",
    "The score provided by Flair is a confidence value within the range [0, 1] for Positive and Negative analysis. We decided to turn the the Negative analysis confidence number into a negative value within the range [-1, 0]. Therefore, the final prediction scores are computed as following:\n",
    "* If confidence score <0 NEGATIVE\n",
    "* If confidence score >0 POSITIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cubic-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_classifier = TextClassifier.load('en-sentiment')\n",
    "def predict(comment):\n",
    "    text = Sentence(comment)\n",
    "    classifier.predict(text)\n",
    "    value = text.labels[0].to_dict()['value'] \n",
    "    if value == 'POSITIVE':\n",
    "        result = text.to_dict()['labels'][0]['confidence']\n",
    "    else:\n",
    "        result = -(text.to_dict()['labels'][0]['confidence'])\n",
    "    return round(result, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "equipped-headquarters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to classify all comments : 58.01 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "flair_scores = []\n",
    "for row in comments_text_df.comment_text:\n",
    "    sent = predict(row)\n",
    "    score = {}\n",
    "    score['Sentiment'] = sent\n",
    "    flair_scores.append(score)\n",
    "\n",
    "#Converting List of Dictionaries into Dataframe\n",
    "flair_scores_df= pd.DataFrame(flair_scores)\n",
    "print('Time to classify all comments : {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "retired-tuition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I didn't really like the song</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alright... I'll take the stairs</td>\n",
       "      <td>-0.841</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a fable on a realistic ground  is a cry of pro...</td>\n",
       "      <td>-0.895</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hope you will exist for many years..</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Would this song exist if he lived on the first...</td>\n",
       "      <td>0.875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  Sentiment  Prediction\n",
       "0                      I didn't really like the song     -1.000          -1\n",
       "1                    alright... I'll take the stairs     -0.841          -1\n",
       "2  a fable on a realistic ground  is a cry of pro...     -0.895          -1\n",
       "3               hope you will exist for many years..      0.997           1\n",
       "4  Would this song exist if he lived on the first...      0.875           1"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flair_rated_comments_df = pd.concat([comments_text_df, flair_scores_df], axis=1)\n",
    "flair_rated_comments_df['Prediction'] = [1 if i>0 else -1 for i in flair_rated_comments_df.Sentiment]\n",
    "flair_rated_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "republican-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving dataframes\n",
    "vader_rated_comments_df.to_csv(data_path+'vader_rated_comments.csv', index = False)\n",
    "textblob_rated_comments_df.to_csv(data_path+'textblob_rated_comments.csv', index = False)\n",
    "flair_rated_comments_df.to_csv(data_path+'flair_rated_comments.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-court",
   "metadata": {},
   "source": [
    "### Discussion of results\n",
    "\n",
    "In this section we discuss the predictions of each model based on certain chosen samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "nuclear-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_combined = vader_rated_comments_df['comment_text'].to_frame().copy()\n",
    "rates_combined['vader_sentiment'] = vader_rated_comments_df['compound'].copy()\n",
    "rates_combined['textblob_sentiment'] = textblob_rated_comments_df['Polarity'].copy()\n",
    "rates_combined['flair_sentiment'] = flair_rated_comments_df['Sentiment'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-september",
   "metadata": {},
   "source": [
    "#### Positve comments\n",
    "For the first positive comment shown below, the three models were able to predict the sentiment correctly. We can observe that Vader and Flair's predictions are much better than Textblob's. This is probably because Vader and Flair models are able to indetify intesinfiers (\"SO MUCH\") and also understand emojis in contexts.\n",
    "\n",
    "The second positive comment is trickier since it contains words that are probably Out Of the Vocabulary (OOV) for the vader and textblob models. However we can observe that Flair works acccurately for this kind of cases of OOV words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "medieval-murder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment_text': 'LOVE THIS SO MUCH ❤️😍',\n",
       " 'vader_sentiment': 0.8374,\n",
       " 'textblob_sentiment': 0.35,\n",
       " 'flair_sentiment': 0.989}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_combined.loc[45484,:].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "abroad-techno",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment_text': 'Oh My Gawd! ♡♡♡ This is so aesthetic and BOBBYYYYY NADEO SARANGHAE!',\n",
       " 'vader_sentiment': 0.0,\n",
       " 'textblob_sentiment': 0.0,\n",
       " 'flair_sentiment': 0.791}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_combined.loc[9372,:].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-watch",
   "metadata": {},
   "source": [
    "#### Negative comments\n",
    "The results in the examples below are quite surprising. The Flair model was accurate about the negativity of the comments, however the vader and textblob models did not perform well. \n",
    "\n",
    "In the case of the vader model, it predicts the first comment as positive and the second as negative. \n",
    "\n",
    "In the case of the textblob model, it is not able to detect the negative comment in any situation and predicts a neutral comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "broadband-establishment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment_text': 'I don’t like his glasses and hair',\n",
       " 'vader_sentiment': 0.3612,\n",
       " 'textblob_sentiment': 0.0,\n",
       " 'flair_sentiment': -1.0}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_combined.loc[45878,:].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "amino-cover",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment_text': \"I don't like this one Tamar\",\n",
       " 'vader_sentiment': -0.2755,\n",
       " 'textblob_sentiment': 0.0,\n",
       " 'flair_sentiment': -1.0}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_combined.loc[23039,:].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-criterion",
   "metadata": {},
   "source": [
    "#### Mixed-sentiment comments\n",
    "\n",
    "In the dataset there are also some comments with mixed feelings as we can see below. \n",
    "\n",
    "In this sorts of cases, we can observe that the Textblob and Vader models perform better than Flair's. Both comments are hard to indentify as they could be seen as neutral or slightly positive/negative. Vader model tend to predict them as sligthly negative, while Textblob predict them as neutral or slightly positive.\n",
    "\n",
    "Flair model performs poorly in both cases predicting a high negative score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "unique-march",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment_text': \"I like the song but I don't like the vocals at the beginning\",\n",
       " 'vader_sentiment': -0.2299,\n",
       " 'textblob_sentiment': 0.0,\n",
       " 'flair_sentiment': -0.998}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_combined.loc[46227,:].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "seasonal-latvia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment_text': \"This is a good song and all, but for me it doesn't have that special feeling.\",\n",
       " 'vader_sentiment': -0.0482,\n",
       " 'textblob_sentiment': 0.5285714285714286,\n",
       " 'flair_sentiment': -0.873}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_combined.loc[9720,:].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-flexibility",
   "metadata": {},
   "source": [
    "#### Missleading comments\n",
    "\n",
    "We call the comments below \"missleading\" since they have a clear positive feeling, but contain expressions that might misslead the prediction of the models.\n",
    "\n",
    "The first comment, is well predicted by the Flair model, but the rest of the models perform poorly. It is worth highlighting that the Vader model predicts quite negative score and it mght be due to the swearing word at the start of the comment.\n",
    "\n",
    "The second comment shows interesting results as it is properly detected by the rule-based models. However, the Flair model is unable to detect the positivism on the comment and performs a poor prediction providing a high negative score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "common-gravity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment_text': 'Damn, I miss this Radiohead',\n",
       " 'vader_sentiment': -0.5106,\n",
       " 'textblob_sentiment': 0.0,\n",
       " 'flair_sentiment': 0.999}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_combined.loc[116,:].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "editorial-defense",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment_text': 'Not bad not bad at all🤔',\n",
       " 'vader_sentiment': 0.1232,\n",
       " 'textblob_sentiment': 0.3499999999999999,\n",
       " 'flair_sentiment': -0.959}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_combined.loc[22408,:].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-pilot",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "After studying the sentiment analysis results of the three models in a few comments, we can draw the following conclusions:\n",
    "\n",
    "* **Emoji comprehension**: Both Vader and Flair models seem to handle them correctly, however TextBlob is unable to process them.\n",
    "\n",
    "* **Out Of the Vocabulary words**: Only the Flair model is able to capture the sentiment of sentences with OOV words.\n",
    "\n",
    "* **Intensifiers**: Both Vader and Flair models seem to handle them correctly, however TextBlob is unable to take them into account.\n",
    "\n",
    "* **Clear Positive comments**: Correctly predicted by all the models\n",
    "\n",
    "* **Clear Negative comments**: Only the Flair model is able to correctly predict them\n",
    "* **Mixed-sentiment comments**: Vader and Textblob seem to handle them correctly, however Flair model seem to perform badly.\n",
    "\n",
    "* **Missleading comments**: Neither of the models is able to analyse correctly the sentiment of such comments in all the cases\n",
    "\n",
    "Based on these conclusions, we think that a combination of the three model predictions could be a good approach to compute a final sentiment score on the comments. For that reason, in the next section we compute an ensemble technique of the three models, in which we will calculate the sentiment score through a weighted average of the previous results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-reporter",
   "metadata": {},
   "source": [
    "### Ensemble technique\n",
    "\n",
    "As we mention in the conclusions, we are going to implement an ensemble technique to obtain the final sentiment scores on the comments. This technique produces an output that is the weighted average of the three models. The weights are assigned as follows:\n",
    "\n",
    "* The Flair model seems to give (generally) better predictions, so we assign it a weight of 50%.\n",
    "* The Vader model seems to perform better than TextBlob, so we assign it a weight of 30%\n",
    "* Finally the TextBlob is assigned with a weight of 20%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "systematic-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_sentiment = []\n",
    "for index, row in rates_combined.iterrows():\n",
    "    score = (row['flair_sentiment']*0.5 + row['vader_sentiment']*0.3+ row['textblob_sentiment']*0.2)/3\n",
    "    ensemble_sentiment.append(score)\n",
    "    \n",
    "ensemble_rates_df = rates_combined['comment_text'].to_frame().copy()\n",
    "ensemble_rates_df['ensemble_sentiment'] = ensemble_sentiment\n",
    "ensemble_rates_df['prediction'] = [1 if i>0 else -1 for i in ensemble_rates_df.ensemble_sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "finite-ghana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>ensemble_sentiment</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I didn't really like the song</td>\n",
       "      <td>-0.185743</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alright... I'll take the stairs</td>\n",
       "      <td>-0.115167</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a fable on a realistic ground  is a cry of pro...</td>\n",
       "      <td>-0.189116</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hope you will exist for many years..</td>\n",
       "      <td>0.243540</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Would this song exist if he lived on the first...</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  ensemble_sentiment  \\\n",
       "0                      I didn't really like the song           -0.185743   \n",
       "1                    alright... I'll take the stairs           -0.115167   \n",
       "2  a fable on a realistic ground  is a cry of pro...           -0.189116   \n",
       "3               hope you will exist for many years..            0.243540   \n",
       "4  Would this song exist if he lived on the first...            0.162500   \n",
       "\n",
       "   prediction  \n",
       "0          -1  \n",
       "1          -1  \n",
       "2          -1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_rates_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-establishment",
   "metadata": {},
   "source": [
    "### Final thoughts\n",
    "\n",
    "After applying the ensemble technique and overviewing the results, a high improvement can be observed.\n",
    "\n",
    "All the simple comments are predicted correctly and this time taking account the negative too. The trickier cases such as mixed-sentiment or missleading comments are not fixed completely, in some cases the final prediction is still wrong but the \"confidence\" on such a result lowers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
